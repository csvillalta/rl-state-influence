{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Influence Calculation Outline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import models\n",
    "import utils\n",
    "\n",
    "from dqn import DQN\n",
    "from circle import CircleEnv\n",
    "\n",
    "# Load configuration for DQN and model\n",
    "gin.parse_config_file('configs/influence/influence.gin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we train our __oracle network__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "episodes = 80\n",
    "steps = 25\n",
    "\n",
    "oracle = DQN()\n",
    "oracle.model.save('oracle_init_model.h5')\n",
    "oracle.target_model.save('oracle_init_target_model.h5')\n",
    "\n",
    "env = CircleEnv()\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    start = state\n",
    "    total_reward = 0\n",
    "    for step in range(steps):\n",
    "#         env.render()\n",
    "        action = oracle.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        oracle.remember((state, action, reward, next_state, episode, step, done))\n",
    "        oracle.replay()\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done or step==steps-1:\n",
    "            print(\"Episode: {:2} | Start: ({:5.2f}, {:5.2f}) | Return: {:5.2f} | e: {:.4f}\".format(episode, start[0], start[1], total_reward, oracle.epsilon))\n",
    "            break\n",
    "\n",
    "        if step%10==0 and step>0:\n",
    "            if oracle.epsilon >= oracle.epsilon_min:\n",
    "                oracle.epsilon *= oracle.epsilon_decay\n",
    "            oracle.update_target_model()\n",
    "\n",
    "oracle.save_training_data('oracle_training_data.h5')\n",
    "oracle.model.save('oracle_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demo our oracle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "demo_env = CircleEnv()\n",
    "utils.demo_agent(oracle, demo_env, 10, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in the necessary oracle data from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_model = 'oracle_model.h5'\n",
    "oracle_init_model = 'oracle_init_model.h5'\n",
    "oracle_init_target_model = 'oracle_init_target_model.h5'\n",
    "oracle_training_data = 'oracle_training_data.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training data and setting up the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_hdf(oracle_training_data, key='training')\n",
    "%time test_data = utils.generate_agent_actions(oracle, n=10000)\n",
    "\n",
    "print(training_data.info())\n",
    "print('='*40)\n",
    "print(test_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect our training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame with every unique state from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_states = training_data[['state_x', 'state_y']].drop_duplicates()\n",
    "print(unique_states.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the meat of the calculations: the __loop__. (Might want to consider creating an *influence function* and call apply on the rows of `unique_states` which may be faster than looping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should return a DataFrame with influences for each unique_state\n",
    "# unique_states.apply(utils.influence, {training_data: training_data, test_data: test_data})\n",
    "# Combine unique_state and influence Series into a DataFrame (probably want this in or influence function...)\n",
    "\n",
    "# TODO: Aggregate by taking the max influence per unique state.\n",
    "for _, state in unique_states.iterrows():\n",
    "    # Why is there a need to drop duplicates here?\n",
    "    state_occurences = training_data[(training_data['state_x'] == state[0]) & \n",
    "                            (training_data['state_y'] == state[1])].drop_duplicates()\n",
    "\n",
    "    for _, state_occurence in state_occurences.iterrows():\n",
    "        episode, step = state_occurence['episode'], state_occurence['step']\n",
    "        \n",
    "        # Every state except those that occurs on or after the above step during the above episode.\n",
    "        full_trace = training_data[(training_data['episode'] != episode) | \n",
    "                          (training_data['step'] < step)]\n",
    "        # Every state except those that occur after the above step during the above episode.\n",
    "        partial_trace = training_data[(training_data['episode'] != episode) | \n",
    "                             (training_data['step'] <= step)]\n",
    "        \n",
    "        # Setup our two agents to train on each of the modified training sets above.\n",
    "        ft_agent = DQN()\n",
    "        ft_agent.model.load_weights(oracle_init_model)\n",
    "        ft_agent.target_model.load_weights(oracle_init_target_model)\n",
    "        \n",
    "        pt_agent = DQN()\n",
    "        pt_agent.model.load_weights(oracle_init_model)\n",
    "        pt_agent.target_model.load_weights(oracle_init_target_model)\n",
    "        \n",
    "        # Train our agents and get their optimal actions on testing data.\n",
    "        utils.train_agent_offline(ft_agent, full_trace.to_numpy())\n",
    "        utils.train_agent_offline(pt_agent, partial_trace.to_numpy())\n",
    "        \n",
    "        ft_agent_actions = utils.get_agent_actions(ft_agent, test_data[['state_x', 'state_y']])\n",
    "        pt_agent_actions = utils.get_agent_actions(pt_agent, test_data[['state_x', 'state_y']])\n",
    "        \n",
    "        # Get accuracies.\n",
    "        ft_agent_acc = utils.agent_accuracy(ft_agent_actions, test_data['action'].to_numpy())\n",
    "        pt_agent_acc = utils.agent_accuracy(pt_agent_actions, test_data['action'].to_numpy())\n",
    "        \n",
    "        # Relative influence.\n",
    "        delta_acc = ft_agent_acc - pt_agent_acc\n",
    "        print(\"Influence of ({:5.2f}, {:5.2f}) at episode {:2}, step {:2}: {:.4f}\".format(state_occurence['state_x'], \n",
    "                                                                        state_occurence['state_y'],\n",
    "                                                                        state_occurence['episode'], \n",
    "                                                                        state_occurence['step'], \n",
    "                                                                        np.round(delta_acc, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing to make sure that our retraining step is training properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_agent = DQN()\n",
    "test_agent.model.load_weights(oracle_init_model)\n",
    "test_agent.target_model.load_weights(oracle_init_target_model)\n",
    "\n",
    "utils.train_agent_offline(test_agent, training_data.to_numpy())\n",
    "test_agent_actions = utils.get_agent_actions(test_agent, test_data[['state_x', 'state_y']])\n",
    "acc = utils.agent_accuracy(test_agent_actions, test_data['action'])\n",
    "print(acc)\n",
    "assert acc == 1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
